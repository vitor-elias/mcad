{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import torch\n",
    "import optuna\n",
    "import joblib\n",
    "import pickle\n",
    "import tifffile\n",
    "import nibabel\n",
    "import scipy.io\n",
    "import pygsp\n",
    "import scipy.ndimage\n",
    "import pyod\n",
    "import warnings\n",
    "import hashlib\n",
    "import sqlite3\n",
    "\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from sklearn.cluster import KMeans, BisectingKMeans, SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.cblof import CBLOF\n",
    "frsource.models.kpca import KPCA\n",
    "\nsource
    "frsourceh import nn, Tensor\n",
    "from torch.nn import Linear, Conv1d, LayerNorm, DataParallel, ReLU, Sequential, Parameter\n",
    "sourcerch_geometric.nn.dense import mincut_pool, dense_mincut_pool\n",
    "sourcerch_geometric.datasets import AttributedGraphDataset\n",
    "sourcerch_geometric.utils import to_networkx, subgraph, to_dense_adj\n",
    "\n",
    "import sensors.nn.models as models\n",
    "import sensors.utils.utils as utils\n",
    "import sensors.utils.fault_detection as fd\n",
    "\n",
    "from sensors.utils.utils import roc_params, compute_auc, get_auc, best_mcc, best_f1score, otsuThresholding\n",
    "from sensors.utils.utils import synthetic_timeseries\n",
    "from sensors.utils.utils import plotly_signal\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "models = reload(models)\n",
    "utils = reload(utils)\n",
    "\n",
    "from pyprojroot import here\n",
    "root_dir = str(here())\n",
    "\n",
    "data_dir = os.path.expanduser('~/data/interim/')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 20})\n",
    "matplotlib.rcParams.update({'font.family': 'DejaVu Serif'})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 \\pm 0.02\n",
      "0.89 \\pm 0.04\n",
      "0.94 \\pm 0.04\n",
      "0.86 \\pm 0.2\n",
      "0.96 \\pm 0.02\n",
      "0.48 \\pm 0.21\n",
      "0.92 \\pm 0.04\n",
      "0.81 \\pm 0.08\n",
      "0.74 \\pm 0.14\n",
      "0.7 \\pm 0.31\n",
      "0.95 \\pm 0.14\n",
      "\n",
      "F1 Score results (for LaTeX):\n",
      "0.75 \\pm 0.06\n",
      "0.64 \\pm 0.06\n",
      "0.75 \\pm 0.11\n",
      "0.73 \\pm 0.27\n",
      "0.78 \\pm 0.05\n",
      "0.32 \\pm 0.1\n",
      "0.67 \\pm 0.09\n",
      "0.51 \\pm 0.1\n",
      "0.46 \\pm 0.14\n",
      "0.47 \\pm 0.25\n",
      "0.9 \\pm 0.19\n",
      "\n",
      "MCC results (for LaTeX):\n",
      "0.71 \\pm 0.07\n",
      "0.59 \\pm 0.07\n",
      "0.71 \\pm 0.12\n",
      "0.69 \\pm 0.32\n",
      "0.75 \\pm 0.06\n",
      "0.14 \\pm 0.16\n",
      "0.62 \\pm 0.1\n",
      "0.43 \\pm 0.12\n",
      "0.37 \\pm 0.16\n",
      "0.4 \\pm 0.32\n",
      "0.88 \\pm 0.23\n"
     ]
    }
   ],
   "source": [
    "parquet_folder = os.path.join(root_dir, 'outputs/testing_mincut')\n",
    "\n",
    "# Load all parquet files starting with \"SB\"\n",
    "parquet_files = glob.glob(os.path.join(parquet_folder, 'SB*.parq'))\n",
    "\n",
    "\n",
    "# Define the desired order\n",
    "method_order = [\"KNN\", \"LOF\", \"COF\", \"CBLOF\", \"KPCA\", \"GAN\", \"AE\", \"GAE\", \"GUNET\", \"MCC\", \"MC\"]\n",
    "\n",
    "# Sort the files based on the specified method order\n",
    "sorted_files = sorted(parquet_files, key=lambda x: method_order.index(os.path.splitext(x)[0].split('_')[-1]))\n",
    "\n",
    "print(\"\\nAUC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    auc_mean = round(df['auc'].mean(), 2)\n",
    "    auc_std = round(df['auc'].std(), 2)\n",
    "    print(f\"{auc_mean} \\\\pm {auc_std}\")\n",
    "\n",
    "# Print F1 Score results\n",
    "print(\"\\nF1 Score results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    f1_mean = round(df['f1score'].mean(), 2)\n",
    "    f1_std = round(df['f1score'].std(), 2)\n",
    "    print(f\"{f1_mean} \\\\pm {f1_std}\")\n",
    "\n",
    "# Print MCC results\n",
    "print(\"\\nMCC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    mcc_mean = round(df['mcc'].mean(), 2)\n",
    "    mcc_std = round(df['mcc'].std(), 2)\n",
    "    print(f\"{mcc_mean} \\\\pm {mcc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89 \\pm 0.01\n",
      "0.88 \\pm 0.09\n",
      "0.73 \\pm 0.2\n",
      "0.89 \\pm 0.09\n",
      "0.72 \\pm 0.15\n",
      "0.56 \\pm 0.1\n",
      "0.88 \\pm 0.04\n",
      "0.75 \\pm 0.14\n",
      "0.89 \\pm 0.02\n",
      "0.88 \\pm 0.13\n",
      "0.93 \\pm 0.05\n",
      "\n",
      "F1 Score results (for LaTeX):\n",
      "0.64 \\pm 0.03\n",
      "0.6 \\pm 0.15\n",
      "0.44 \\pm 0.17\n",
      "0.64 \\pm 0.13\n",
      "0.47 \\pm 0.18\n",
      "0.22 \\pm 0.04\n",
      "0.55 \\pm 0.09\n",
      "0.4 \\pm 0.14\n",
      "0.65 \\pm 0.06\n",
      "0.64 \\pm 0.2\n",
      "0.66 \\pm 0.11\n",
      "\n",
      "MCC results (for LaTeX):\n",
      "0.64 \\pm 0.02\n",
      "0.59 \\pm 0.15\n",
      "0.41 \\pm 0.22\n",
      "0.64 \\pm 0.13\n",
      "0.42 \\pm 0.21\n",
      "0.14 \\pm 0.07\n",
      "0.56 \\pm 0.09\n",
      "0.4 \\pm 0.14\n",
      "0.65 \\pm 0.06\n",
      "0.63 \\pm 0.21\n",
      "0.64 \\pm 0.12\n"
     ]
    }
   ],
   "source": [
    "parquet_folder = os.path.join(root_dir, 'outputs/testing_mincut')\n",
    "\n",
    "parquet_files = glob.glob(os.path.join(parquet_folder, 'ST*.parq'))\n",
    "\n",
    "\n",
    "# Define the desired order\n",
    "method_order = [\"KNN\", \"LOF\", \"COF\", \"CBLOF\", \"KPCA\", \"GAN\", \"AE\", \"GAE\", \"GUNET\", \"MCC\", \"MC\", \"MCconv\"]\n",
    "\n",
    "# Sort the files based on the specified method order\n",
    "sorted_files = sorted(parquet_files, key=lambda x: method_order.index(os.path.splitext(x)[0].split('_')[-1]))[0:-1]\n",
    "\n",
    "print(\"\\nAUC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    auc_mean = round(df['auc'].mean(), 2)\n",
    "    auc_std = round(df['auc'].std(), 2)\n",
    "    print(f\"{auc_mean} \\\\pm {auc_std}\")\n",
    "\n",
    "# Print F1 Score results\n",
    "print(\"\\nF1 Score results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    f1_mean = round(df['f1score'].mean(), 2)\n",
    "    f1_std = round(df['f1score'].std(), 2)\n",
    "    print(f\"{f1_mean} \\\\pm {f1_std}\")\n",
    "\n",
    "# Print MCC results\n",
    "print(\"\\nMCC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    mcc_mean = round(df['mcc'].mean(), 2)\n",
    "    mcc_std = round(df['mcc'].std(), 2)\n",
    "    print(f\"{mcc_mean} \\\\pm {mcc_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUC results (for LaTeX):\n",
      "0.86 \\pm 0.04\n",
      "0.86 \\pm 0.04\n",
      "0.84 \\pm 0.08\n",
      "0.85 \\pm 0.09\n",
      "0.82 \\pm 0.03\n",
      "0.51 \\pm 0.07\n",
      "0.91 \\pm 0.04\n",
      "0.64 \\pm 0.05\n",
      "0.69 \\pm 0.03\n",
      "0.92 \\pm 0.09\n",
      "1.0 \\pm 0.0\n",
      "\n",
      "F1 Score results (for LaTeX):\n",
      "0.35 \\pm 0.05\n",
      "0.37 \\pm 0.06\n",
      "0.36 \\pm 0.09\n",
      "0.38 \\pm 0.1\n",
      "0.3 \\pm 0.04\n",
      "0.15 \\pm 0.03\n",
      "0.45 \\pm 0.07\n",
      "0.19 \\pm 0.04\n",
      "0.22 \\pm 0.05\n",
      "0.7 \\pm 0.18\n",
      "1.0 \\pm 0.0\n",
      "\n",
      "MCC results (for LaTeX):\n",
      "0.39 \\pm 0.04\n",
      "0.37 \\pm 0.06\n",
      "0.37 \\pm 0.09\n",
      "0.4 \\pm 0.1\n",
      "0.31 \\pm 0.03\n",
      "0.09 \\pm 0.05\n",
      "0.45 \\pm 0.07\n",
      "0.19 \\pm 0.03\n",
      "0.24 \\pm 0.04\n",
      "0.71 \\pm 0.18\n",
      "1.0 \\pm 0.0\n"
     ]
    }
   ],
   "source": [
    "parquet_folder = os.path.join(root_dir, 'outputs/testing_mincut')\n",
    "\n",
    "# Load all parquet files starting with \"SB\"\n",
    "parquet_files = glob.glob(os.path.join(parquet_folder, 'SI*.parq'))\n",
    "\n",
    "\n",
    "# Define the desired order\n",
    "method_order = [\"KNN\", \"LOF\", \"COF\", \"CBLOF\", \"KPCA\", \"GAN\", \"AE\", \"GAE\", \"GUNET\", \"MCC\", \"MC\"]\n",
    "\n",
    "# Sort the files based on the specified method order\n",
    "sorted_files = sorted(parquet_files, key=lambda x: method_order.index(os.path.splitext(x)[0].split('_')[-1]))\n",
    "\n",
    "print(\"\\nAUC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    auc_mean = round(df['auc'].mean(), 2)\n",
    "    auc_std = round(df['auc'].std(), 2)\n",
    "    print(f\"{auc_mean} \\\\pm {auc_std}\")\n",
    "\n",
    "# Print F1 Score results\n",
    "print(\"\\nF1 Score results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    f1_mean = round(df['f1score'].mean(), 2)\n",
    "    f1_std = round(df['f1score'].std(), 2)\n",
    "    print(f\"{f1_mean} \\\\pm {f1_std}\")\n",
    "\n",
    "# Print MCC results\n",
    "print(\"\\nMCC results (for LaTeX):\")\n",
    "for file_path in sorted_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    mcc_mean = round(df['mcc'].mean(), 2)\n",
    "    mcc_std = round(df['mcc'].std(), 2)\n",
    "    print(f\"{mcc_mean} \\\\pm {mcc_std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsensors",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
